{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1BxZ5hHmwy5zBu_lMYcJxg8E2k1_6D8Bs","authorship_tag":"ABX9TyMWhiydx7m2q/9MnMPAC57G"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Kf83rdHe6o-c","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","### 1 or 2###\n","# 1\n","\"\"\"\n","try:\n","  !pip install -q tf-nightly\n","except Exception:\n","  pass\n","\"\"\"\n","# end 1\n","\n","# 2\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","# end 2\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import io\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","SOS = '<start>'\n","EOS = '<end>'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mIxyI3QrlG3","colab_type":"code","colab":{}},"source":["def tokenize(lang, target_vocab_size, vocab_filename = None):\n","  \"\"\"\n","  Create a custom subwords tokenizer from the training dataset (lang).\n","  The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary.\n","  \"\"\"\n","  tokenizer = None\n","  if vocab_filename :\n","    if os.path.exists(vocab_filename):\n","      # charger a partir du fichier specifié (pour gagner en temps)\n","      tokenizer = tfds.features.text.SubwordTextEncoder.load_from_file(vocab_filename)\n","    else :\n","      # construire et charger dans le fichier specifié (pour des futures chargements)\n","      tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(lang, target_vocab_size = target_vocab_size)\n","      tokenizer.save_to_file(vocab_filename)\n","  else :\n","    # construire juste (deprecié)\n","    tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(lang, target_vocab_size = target_vocab_size)\n","  \"\"\"\n","  Exemple : english corpus\n","\n","  sample_string = 'Transformer is awesome.'\n","\n","  tokenized_string = tokenizer.encode(sample_string)\n","  print(tokenized_string) # [7915, 1248, 7946, 7194, 13, 2799, 7877]\n","\n","  original_string = tokenizer.decode(tokenized_string)\n","  print(original_string) # Transformer is awesome.\n","\n","  for ts in tokenized_string:\n","    print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))\n","\n","  7915 ----> T\n","  1248 ----> ran\n","  7946 ----> s\n","  7194 ----> former \n","  13 ----> is \n","  2799 ----> awesome\n","  7877 ----> .\n","  \"\"\"\n","  return tokenizer\n","\n","def create_dataset(path, sentencesSeparator):\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","  lines = [e.split(sentencesSeparator) for e in lines]\n","  return lines\n","\n","def py_function(tokenizer_input_lang, tokenizer_target_lang):\n","    \n","  def encode(input_sample, target_sample):\n","    \"\"\"\n","    Add a start and end token to the input and target.\n","    encode(start) = vocab_size\n","    encode(end) = vocab_size + 1\n","    \"\"\"\n","    #input_sample = [tokenizer_input_lang.vocab_size] + tokenizer_input_lang.encode(input_sample.numpy()) + [tokenizer_input_lang.vocab_size+1]\n","    #target_sample = [tokenizer_target_lang.vocab_size] + tokenizer_target_lang.encode(target_sample.numpy()) + [tokenizer_target_lang.vocab_size+1]\n","      \n","    return input_sample, target_sample\n","\n","  def tf_encode(input_lang, target_lang) :\n","    \"\"\"\n","    You want to use Dataset.map to apply this function to each element of the dataset. Dataset.map runs in graph mode.\n","    Graph tensors do not have a value.\n","    In graph mode you can only use TensorFlow Ops and functions.\n","    So you can't .map this function directly: You need to wrap it in a tf.py_function. \n","    The tf.py_function will pass regular tensors (with a value and a .numpy() method to access it), to the wrapped python function.\n","    \"\"\"\n","    result_input_lang, result_target_lang = tf.py_function(encode, [input_lang, target_lang], [tf.int32, tf.int32])\n","    result_input_lang.set_shape([None])\n","    result_target_lang.set_shape([None])\n","    return result_input_lang, result_target_lang \n","    \n","  return tf_encode\n","\n","def filter_max_length(x, y, max_length = 50):\n","  return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cD2nJZlXsey6","colab_type":"code","colab":{}},"source":["def load_dataset(path, sentencesSeparator, input_vocab_filename, target_vocab_filename, MAX_LENGTH, BUFFER_SIZE, BATCH_SIZE, \n","                  target_vocab_size, test_size, val_size, num_examples = None, shift_EOS=False):\n","  \n","  lines = create_dataset(path, sentencesSeparator)\n","  \n","  dataset = tf.data.Dataset.from_tensor_slices(lines)\n","\n","  tokenizer_input_lang = tokenize((text.numpy() for text, _ in dataset), vocab_filename = input_vocab_filename, target_vocab_size = target_vocab_size)\n","  tokenizer_target_lang = tokenize((text.numpy() for _, text in dataset), vocab_filename = target_vocab_filename, target_vocab_size = target_vocab_size)\n","  \n","  # Add a start and end token to the input and target.\n","  \"\"\"\n","  encode(start) = vocab_size\n","  encode(end) = vocab_size + 1\n","  \"\"\"\n","  train_examples = [\n","                    [\n","                      [tokenizer_input_lang.vocab_size] + tokenizer_input_lang.encode(e[0]) + [tokenizer_input_lang.vocab_size+1], \n","                      [tokenizer_target_lang.vocab_size] + tokenizer_target_lang.encode(e[1]) + [tokenizer_target_lang.vocab_size+1]\n","                    ]\n","                    for e in lines\n","                   ]\n","  \"\"\"\n","  train_examples = [\n","                    [\n","                      tokenizer_input_lang.encode(e[0]), \n","                      tokenizer_target_lang.encode(e[1])\n","                    ]\n","                    for e in lines\n","                   ]\n","  \"\"\"\n","  max_length = max([max(len(e[0]), len(e[1])) for e in train_examples])\n","  max_length = max(MAX_LENGTH, max_length)\n","  \n","  np.random.shuffle(train_examples)\n","\n","  if num_examples :\n","      train_examples = train_examples[:num_examples]\n","\n","  def pad(sample):\n","    a, b, end = sample[0], sample[1], []\n","    l1, l2 = len(a), len(b)\n","    #max_length = max(l1, l2)\n","    if shift_EOS :\n","        end = [a[-1]]\n","        a = a[:l1-1]\n","        b = b[:l2-1]\n","    a = a + [0 for _ in range(max_length - l1)] + end\n","    b = b + [0 for _ in range(max_length - l2)] + end\n","    return [a, b]\n","\n","  train_examples = [pad(e) for e in train_examples]\n","\n","  test_examples, val_examples = None, None\n","  if 0 < test_size :\n","    # Une exception est levée si tres peu de données\n","    try :\n","      train_examples, test_examples = train_test_split(train_examples, test_size = test_size)\n","    except :\n","      pass\n","  if 0 < val_size :\n","    try :\n","      train_examples, val_examples = train_test_split(train_examples, test_size = val_size)\n","    except :\n","      pass\n","\n","  def tf_rechape(inp, tar):\n","    inp.set_shape([None])\n","    tar.set_shape([None])\n","    return inp, tar\n","\n","  def from_tf_dataset(examples, train_one = False):\n","    targ_lang, inp_lang = zip(*examples)\n","    targ_lang = tf.keras.preprocessing.sequence.pad_sequences(targ_lang, padding='post')\n","    inp_lang = tf.keras.preprocessing.sequence.pad_sequences(inp_lang, padding='post')\n","    dataset = tf.data.Dataset.from_tensor_slices((inp_lang, targ_lang))\n","    #dataset = dataset.map(tf_rechape)\n","    dataset = dataset.map(py_function(tokenizer_input_lang, tokenizer_target_lang))\n","    \n","    if train_one :\n","      # cache the dataset to memory to get a speedup while reading from it.\n","      dataset = dataset.cache()\n","      # shuffle\n","      BUFFER_SIZE = len(targ_lang)\n","      dataset = dataset.shuffle(BUFFER_SIZE)\n","      # Pad and batch examples together\n","      dataset = dataset.padded_batch(BATCH_SIZE)\n","      \"\"\"\n","      Creates a Dataset that prefetches elements from this dataset.\n","      Most dataset input pipelines should end with a call to prefetch. \n","      This allows later elements to be prepared while the current element is being processed. \n","      This often improves latency and throughput, at the cost of using additional memory to store \n","      prefetched elements.\n","      \"\"\"\n","      dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","    else :\n","      # Pad and batch examples together\n","      dataset = dataset.padded_batch(BATCH_SIZE)\n","    return dataset\n","\n","  train_dataset = from_tf_dataset(train_examples, train_one = True)\n","  val_dataset = from_tf_dataset(val_examples) if val_examples else None\n","  test_dataset = from_tf_dataset(test_examples) if test_examples else None\n"," \n","  metadata = {\"max_length\" : max_length, 'buffer_size' : BUFFER_SIZE }\n","  \n","  return train_dataset, val_dataset, test_dataset, tokenizer_input_lang, tokenizer_target_lang, metadata"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EnMixjQG7MJm","colab_type":"code","colab":{}},"source":["# Positional encoding\n","\n","def positional_encoding(position, d_model):\n","\n","  \"\"\"\n","  Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about \n","  the relative position of the words in the sentence.\n","  \n","  The positional encoding vector is added to the embedding vector. \n","\n","  Given an input sequence x, lenght(x) = seq_len : \n","  X = Z + P avec Z = embedding(x) et P = positional_encoding(seq_len, embedding_dim)\n","  X.shape = Z.shape = P.shape = (..., batch_size, seq_len, embedding_dim) \n","\n","  Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. \n","  But the embeddings do not encode the relative position of words in a sentence. \n","  So after adding the positional encoding, words will be closer to each other based on the similarity of their meaning and \n","  their position in the sentence, in the d-dimensional space.\n","\n","  Voir la publication attachée à ce notebook pour plus de détail (The formula for calculating the positional encoding)\n","  \"\"\"\n","\n","  def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","  \n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n","  \n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","  pos_encoding = angle_rads[np.newaxis, ...]\n","    \n","  return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","\"\"\"\n","pos_encoding = positional_encoding(50, 512)\n","print (pos_encoding.shape)\n","\n","plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n","plt.xlabel('Depth')\n","plt.xlim((0, 512))\n","plt.ylabel('Position')\n","plt.colorbar()\n","plt.show()\n","\"\"\"\n","\n","# Masking\n","\n","def create_padding_mask(seq):\n","  \"\"\"\n","  seq.shape = (batch_size, seq_len)\n","  Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. \n","  The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n","  Cette methode met en effet tout les elements non nuls de la sequence de depart à 0, et les elements nuls à 1.\n","  \"\"\"\n","  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)  \n","  # add extra dimensions to add the padding to the attention logits.\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","def create_look_ahead_mask(size):\n","  \"\"\"\n","  size = seq_len\n","  The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n","  This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, \n","  second and the third word will be used and so on.\n","  Cette methode retourne une matrice dont tout les elements de la partie triangulaire inferieure sont nuls, y compris ceux de la diagonale; et les\n","  les elements de la partie triangulaire superieure valent tous 1.\n","  \"\"\"\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask  # (seq_len, seq_len) \n","\n","# Scaled dot product attention\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","  # Voir la publication attachée à ce notebook pour plus de détail\n","  \"\"\"\n","  Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","  \n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","\n","  En pratique : \n","    - X = embedding(x = input_sequence) + positional_encoding(seq_len, embedding_dim) \n","      X.shape = (batch_size, seq_len, embedding_dim) et lenght(x) = seq_len\n","\n","    - Wq.shape = (embedding_dim, depth_q)\n","    - Wk.shape = (embedding_dim, depth_k)\n","    - Wv.shape = (embedding_dim, depth_v)\n","\n","    - q = X * Wq  # (seq_len, depth_q)\n","    - k = X * Wk  # (seq_len, depth_k) avec depth_q = depth_k\n","    - v = X * Wv  # (seq_len, depth_v)\n","    \n","  Returns:\n","    output, attention_weights\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k), en pratique : (seq_len, seq_len)\n","  \n","  # scale matmul_qk\n","  \"\"\"\n","  The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, \n","  the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.\n","\n","  For example, consider that Q and K have a mean of 0 and variance of 1. \n","  Their matrix multiplication will have a mean of 0 and variance of dk. \n","  Hence, square root of dk is used for scaling (and not any other number) because the matmul of Q and K should have a mean \n","  of 0 and variance of 1, and you get a gentler softmax.\n","  \"\"\"\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    \"\"\"\n","    The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix\n","    multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative \n","    inputs to softmax are near zero in the output.\n","    \"\"\"\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k), en pratique : (seq_len, seq_len)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v), en pratique : (seq_len, depth_v)\n","\n","  \"\"\"\n","  As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n","  The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want \n","  to focus on are kept as-is and the irrelevant words are flushed out.\n","  \"\"\"\n","\n","  return output, attention_weights\n","\n","# Multi-head attention\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","  # Voir la publication attachée à ce notebook pour plus de détail\n","  \"\"\"\n","  Multi-head attention consists of four parts:\n","  - Linear layers and split into heads.\n","  - Scaled dot-product attention.\n","  - Concatenation of heads.\n","  - Final linear layer.\n","\n","  - d_model = num_heads * depth\n","\n","  Each multi-head attention block gets three inputs; Q (query), K (key), V (value). \n","  These are put through linear (Dense) layers and split up into multiple heads.\n","\n","  The scaled_dot_product_attention defined above is applied to each head (broadcasted for efficiency). \n","  An appropriate mask must be used in the attention step. \n","  The attention output for each head is then concatenated (using tf.transpose, and tf.reshape) and put through a final Dense layer.\n","\n","  Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to \n","  information at different positions from different representational spaces. \n","  After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with \n","  full dimensionality.\n","  \"\"\"\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","    \n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads\n","    \n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","    \n","    self.dense = tf.keras.layers.Dense(d_model)\n","        \n","  def split_heads(self, x, batch_size):\n","    \"\"\"\n","    Split the last dimension (d_model = num_heads * depth) into (num_heads, depth).\n","    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","  def call(self, v, k, q, mask):\n","    # En pratique : v = k = q = X = Z + positional_encoding(Z) avec Z = embedding(x = input_sequence)  \n","    # X.shape = (batch_size, seq_len, embedding_dim) et lenght(x) = seq_len\n","\n","    # ici embedding_dim = d_model / num_heads = depth\n","\n","    batch_size = tf.shape(q)[0]\n","    \n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","   \n","    # d_model = num_heads * depth\n","    # split_heads => (batch_size, num_heads, seq_len, depth)\n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","    \n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","    \n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","    \n","    # num_heads * depth = d_model\n","    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","    return output, attention_weights\n","\n","# Point wise feed forward network\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","  \"\"\"\n","  Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n","  \"\"\"\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])\n","\n","### Encoder and decoder ###\n","# Voir la publication attachée à ce notebook pour plus de détail\n","\"\"\"\n","The transformer model follows the same general pattern as a standard sequence to sequence with attention model : \n","- The input sentence is passed through N encoder layers that generates an output for each word/token in the sequence.\n","- The decoder attends on the encoder's output and its own input (self-attention) to predict the next word.\n","\"\"\"\n","\n","# Encoder layer\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","  \"\"\"\n","  Encoder layer consists of sublayers:\n","  - Multi-head attention (with padding mask)\n","  - Point wise feed forward networks.\n","\n","  Each of these sublayers has a residual connection around it followed by a layer normalization. \n","  Residual connections help in avoiding the vanishing gradient problem in deep networks.\n","\n","  The output of each sublayer is LayerNorm(x + Sublayer(x)). \n","  The normalization is done on the d_model (last) axis. There are N encoder layers in the transformer.\n","  \"\"\"\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, training, mask):\n","\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    return out2\n","\n","# Decoder layer\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","  \"\"\"\n","  Decoder layer consists of sublayers:\n","  - Masked multi-head attention (with look ahead mask and padding mask)\n","  - Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. \n","    Q (query) receives the output from the masked multi-head attention sublayer.\n","  - Point wise feed forward networks\n","\n","  Each of these sublayers has a residual connection around it followed by a layer normalization. \n","  The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n","\n","  There are N decoder layers in the transformer.\n","\n","  As Q receives the output from decoder's first attention block, and K receives the encoder output, \n","  the attention weights represent the importance given to the decoder's input based on the encoder's output. \n","  In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. \n","  \"\"\"\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n"," \n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","    # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","    attn1 = self.dropout1(attn1, training=training)\n","    out1 = self.layernorm1(attn1 + x)\n","    \n","    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","    attn2 = self.dropout2(attn2, training=training)\n","    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","    \n","    return out3, attn_weights_block1, attn_weights_block2\n","\n","\n","# Encoder\n","\n","class Encoder(tf.keras.layers.Layer):\n","  \"\"\"\n","  The Encoder consists of:\n","  - Input Embedding\n","  - Positional Encoding\n","  - N encoder layers\n","  \n","  The input is put through an embedding which is summed with the positional encoding. \n","  The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder.\n","  \"\"\"\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    \n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","    \n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","  \n","    self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    \n","    # adding embedding and position encoding.\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","    \n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","    \n","    return x  # (batch_size, input_seq_len, d_model)\n","\n","# Decoder\n","\n","class Decoder(tf.keras.layers.Layer):\n","  \"\"\"\n","  The Decoder consists of:\n","  - Output Embedding\n","  - Positional Encoding\n","  - N decoder layers\n","\n","  The target is put through an embedding which is summed with the positional encoding. \n","  The output of this summation is the input to the decoder layers. \n","  The output of the decoder is the input to the final linear layer.\n","  \"\"\"\n","  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    \n","    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","    \n","    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","    \n","    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    \n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                             look_ahead_mask, padding_mask)\n","      \n","      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","    \n","    # x.shape == (batch_size, target_seq_len, d_model)\n","    return x, attention_weights\n","\n","# Create the Transformer\n","\n","class Transformer(tf.keras.Model):\n","  \"\"\"\n","  Transformer consists of the encoder, decoder and a final linear layer. \n","  The output of the decoder is the input to the linear layer and its output is returned.\n","  \"\"\"\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","  def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","\n","    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","    \n","    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","    \n","    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","    \n","    return final_output, attention_weights\n","\n","# Optimizer\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  \"\"\"Use the Adam optimizer with a custom learning rate scheduler according to the formula in the paper\"\"\"\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","      \n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","      \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","      \n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","def create_masks(inp, tar):\n","  # Encoder padding mask\n","  enc_padding_mask = create_padding_mask(inp)\n","    \n","  # Used in the 2nd attention block in the decoder.\n","  # This padding mask is used to mask the encoder outputs.\n","  dec_padding_mask = create_padding_mask(inp)\n","    \n","  # Used in the 1st attention block in the decoder.\n","  # It is used to pad and mask future tokens in the input received by the decoder.\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","    \n","  return enc_padding_mask, combined_mask, dec_padding_mask\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ey_KK7j-Vru","colab_type":"code","colab":{}},"source":["input_language = \"Francais\"\n","target_language = \"Anglais\"\n","test_size = 0\n","val_size = 0.1\n","num_examples = None\n","MAX_LENGTH = 40 \n","BUFFER_SIZE = 20000\n","BATCH_SIZE = 64\n","target_vocab_size = 2**13\n","path_to_dataset = \"/content/drive/My Drive/datasets/YourVersion/txts/\"+input_language+\"_\" +target_language +\"_pnb1.txt\"\n","vocab_dir = \"/content/drive/My Drive/datasets/YourVersion/vocab_dir\"\n","dir_model = \"/content/drive/My Drive/datasets/YourVersion/dir_model\"\n","checkpoint_dir =  \"/content/drive/My Drive/datasets/YourVersion/checkpoint_dir\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIGlRx1J-YNe","colab_type":"code","colab":{}},"source":["train_dataset, val_dataset, test_dataset, tokenizer_input_lang, tokenizer_target_lang, metadata = load_dataset(\n","      path = path_to_dataset, \n","      sentencesSeparator = '__SEPARATOR__',\n","      input_vocab_filename = vocab_dir + \"/\"+ input_language, \n","      target_vocab_filename = vocab_dir + \"/\"+ target_language, \n","      MAX_LENGTH = MAX_LENGTH, \n","      BUFFER_SIZE = BUFFER_SIZE, \n","      BATCH_SIZE = BATCH_SIZE, \n","      target_vocab_size = target_vocab_size,\n","      test_size = test_size, \n","      val_size = val_size,\n","      num_examples = num_examples\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bELPaK886UUh","colab_type":"code","colab":{}},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_input_lang.vocab_size + 2\n","target_vocab_size = tokenizer_target_lang.vocab_size + 2\n","dropout_rate = 0.1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVqlIj4p9m9N","colab_type":"code","colab":{}},"source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnzQrA9s9nGS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":296},"outputId":"e2c4d17f-7a72-4ec2-8aea-8a7a7a0d510f","executionInfo":{"status":"ok","timestamp":1586251980561,"user_tz":-120,"elapsed":2369,"user":{"displayName":"pascal notsawo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB3JNptAHBvU_EJOwzOH0F7I9P3NvG7oF-0Ae9=s64","userId":"02674667745815558765"}}},"source":["temp_learning_rate_schedule = CustomSchedule(d_model)\n","\n","plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Train Step')"]},"metadata":{"tags":[]},"execution_count":51},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"8ovZ3dYx9cTJ","colab_type":"code","colab":{}},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VlUhFbXC9WzY","colab_type":"code","colab":{}},"source":["def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  \n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rII8K8Pa6jQl","colab_type":"code","colab":{}},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBDvFIr87pJm","colab_type":"code","colab":{}},"source":["transformer = Transformer(num_layers, d_model, num_heads, dff,\n","                          input_vocab_size, target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxlFPahw7pfg","colab_type":"code","colab":{}},"source":["def create_masks(inp, tar):\n","  # Encoder padding mask\n","  enc_padding_mask = create_padding_mask(inp)\n","  \n","  # Used in the 2nd attention block in the decoder.\n","  # This padding mask is used to mask the encoder outputs.\n","  dec_padding_mask = create_padding_mask(inp)\n","  \n","  # Used in the 1st attention block in the decoder.\n","  # It is used to pad and mask future tokens in the input received by \n","  # the decoder.\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","  return enc_padding_mask, combined_mask, dec_padding_mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6hApCZC8cYn","colab_type":"code","colab":{}},"source":["#checkpoint_path = \"./checkpoints/train\"\n","checkpoint_path = checkpoint_dir + \"/\" + input_language + \"_\" + target_language \n","\n","ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"08gDMtKG8j89","colab_type":"code","colab":{}},"source":["# The @tf.function trace-compiles train_step into a TF graph for faster\n","# execution. The function specializes to the precise shape of the argument\n","# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n","# batch sizes (the last batch is smaller), use input_signature to specify\n","# more generic shapes.\n","\n","train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","]\n","\n","#@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","  \n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","  \n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inp, tar_inp, \n","                                 True, \n","                                 enc_padding_mask, \n","                                 combined_mask, \n","                                 dec_padding_mask)\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_variables)    \n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  \n","  train_loss(loss)\n","  train_accuracy(tar_real, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-jAH7kjP_n1h","colab_type":"code","colab":{}},"source":["EPOCHS = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FRx4Q8_8zDJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"d361195a-a342-4c12-c301-31f3c77512c6","executionInfo":{"status":"error","timestamp":1586252137835,"user_tz":-120,"elapsed":2397,"user":{"displayName":"pascal notsawo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB3JNptAHBvU_EJOwzOH0F7I9P3NvG7oF-0Ae9=s64","userId":"02674667745815558765"}}},"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","  \n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  \n","  # inp -> portuguese, tar -> english\n","  for (batch, (inp, tar)) in enumerate(train_dataset):\n","    train_step(inp, tar)\n","    \n","    if batch % 50 == 0:\n","      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","      \n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    \n","  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","\n","  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"],"execution_count":61,"outputs":[{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-0133f04feef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-59-55cf610ef8b6>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(inp, tar)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                  \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                  \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                  dec_padding_mask)\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-e9dc52deae13>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-e9dc52deae13>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;31m# adding embedding and position encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int32'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         result = _clip(\n\u001b[0;32m--> 137\u001b[0;31m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4520\u001b[0m     \u001b[0;31m# TODO(apassos) find a less bad way of detecting resource variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[0;31m# without introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4523\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4524\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[0;34m(self, indices, name)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[0;32m--> 676\u001b[0;31m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[0;34m(resource, indices, dtype, batch_dims, validate_indices, name)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: indices[56,0] = 8633 is not in [0, 8113) [Op:ResourceGather]"]}]},{"cell_type":"code","metadata":{"id":"aoi-zcrh86lj","colab_type":"code","colab":{}},"source":["def evaluate(inp_sentence):\n","  #tokenizer_input_lang, tokenizer_target_lang\n","  start_token = [tokenizer_input_lang.vocab_size]\n","  end_token = [tokenizer_input_lang.vocab_size + 1]\n","  \n","  # inp sentence is portuguese, hence adding the start and end token\n","  inp_sentence = start_token + tokenizer_input_lang.encode(inp_sentence) + end_token\n","  encoder_input = tf.expand_dims(inp_sentence, 0)\n","  \n","  # as the target is english, the first word to the transformer should be the\n","  # english start token.\n","  decoder_input = [tokenizer_target_lang.vocab_size]\n","  output = tf.expand_dims(decoder_input, 0)\n","    \n","  for i in range(MAX_LENGTH):\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        encoder_input, output)\n","  \n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions, attention_weights = transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","    \n","    # select the last word from the seq_len dimension\n","    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    \n","    # return the result if the predicted_id is equal to the end token\n","    if predicted_id == tokenizer_target_lang.vocab_size+1:\n","      return tf.squeeze(output, axis=0), attention_weights\n","    \n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0), attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzHXaBh487sy","colab_type":"code","colab":{}},"source":["def translate(sentence, plot=''):\n","  result, attention_weights = evaluate(sentence)\n","  \n","  predicted_sentence = tokenizer_target_lang.decode([i for i in result if i < tokenizer_target_lang.vocab_size])  \n","\n","  print('Input: {}'.format(sentence))\n","  print('Predicted translation: {}'.format(predicted_sentence))\n","  \n","  if plot:\n","    plot_attention_weights(attention_weights, sentence, result, plot)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eybZh-WDAmza","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":394},"outputId":"54aad317-a2a3-46b4-99cc-35f38822df2f","executionInfo":{"status":"ok","timestamp":1586252665576,"user_tz":-120,"elapsed":4796,"user":{"displayName":"pascal notsawo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB3JNptAHBvU_EJOwzOH0F7I9P3NvG7oF-0Ae9=s64","userId":"02674667745815558765"}}},"source":["translate(sentence = \"je suis l'alpha et l'omega.\", plot='')\n","translate(sentence = 'le premier et le dernier.', plot='')\n","translate(sentence = 'le commencement et la fin', plot='')\n","translate(sentence = 'il a venge le sang de ses serviteurs repandu par ses mains.', plot='')\n","translate(sentence = \"et ils dirent une seconde fois alleluia.\", plot='')"],"execution_count":64,"outputs":[{"output_type":"stream","text":["Input: je suis l'alpha et l'omega.\n","Predicted translation: performedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformed\n","Input: le premier et le dernier.\n","Predicted translation: performedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformed\n","Input: le commencement et la fin\n","Predicted translation: performedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformed\n","Input: il a venge le sang de ses serviteurs repandu par ses mains.\n","Predicted translation: performedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformed\n","Input: et ils dirent une seconde fois alleluia.\n","Predicted translation: performedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformedperformed\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(104,), dtype=int32, numpy=\n","array([8633, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131, 6131,\n","       6131, 6131, 6131, 6131, 6131], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"zVdnvhhW7zji","colab_type":"code","colab":{}},"source":["class Trainer() :\n","  def __init__(self, \n","    input_language, \n","    target_language, \n","    path_to_dataset, \n","    vocab_dir,\n","    dir_model,\n","    checkpoint_dir,\n","\n","    # to load dataset\n","    # todo : faire en sorte que le dataset ne soit charger que si on fait le build, et non la restauration\n","    # ie : faire en sorte que ces parametres soient construire et sauvegarder lors du build\n","    sentencesSeparator = '__SEPARATOR__',\n","    \n","    test_size = 0, # le model est évalué directement lors du train, pas besoin des données de test comme dans l'ancien model (v1.0)\n","    val_size = 0.1, # nous avons besoin des données de validation\n","    num_examples = None,\n","    MAX_LENGTH = 40, \n","    BUFFER_SIZE = 20000, \n","    BATCH_SIZE = 64, \n","    target_vocab_size = 2**13\n","    ):\n","\n","    self.input_language = input_language\n","    self.target_language = target_language\n","    self.path_to_dataset = path_to_dataset\n","    self.vocab_dir = vocab_dir\n","    self.dir_model = dir_model\n","    self.checkpoint_dir = checkpoint_dir\n","    self.sentencesSeparator = sentencesSeparator\n","    self.num_examples = num_examples \n","\n","    self.test_size = test_size\n","    self.val_size = val_size\n","    self.num_examples = num_examples\n","    self.MAX_LENGTH = MAX_LENGTH \n","    self.BUFFER_SIZE = BUFFER_SIZE \n","    self.BATCH_SIZE = BATCH_SIZE \n","    self.target_vocab_size = target_vocab_size\n","\n","    self.train_dataset, self.val_dataset, self.test_dataset, self.tokenizer_input_lang, self.tokenizer_target_lang, metadata = load_dataset(\n","      path = self.path_to_dataset, \n","      sentencesSeparator = self.sentencesSeparator,\n","      input_vocab_filename = self.vocab_dir + \"/\"+ self.input_language, \n","      target_vocab_filename = self.vocab_dir + \"/\"+ self.target_language, \n","      MAX_LENGTH = self.MAX_LENGTH, \n","      BUFFER_SIZE = self.BUFFER_SIZE, \n","      BATCH_SIZE = self.BATCH_SIZE, \n","      target_vocab_size = self.target_vocab_size,\n","      test_size = self.test_size, \n","      val_size = self.val_size,\n","      num_examples = self.num_examples\n","    )\n","\n","    self.MAX_LENGTH = metadata[\"max_length\"]\n","    self.BUFFER_SIZE = metadata[\"buffer_size\"]\n","\n","    self.input_vocab_size = tokenizer_input_lang.vocab_size + 2\n","    self.target_vocab_size = tokenizer_target_lang.vocab_size + 2\n","  \n","  def build(self, num_layers = 6, d_model = 512, dff = 2048, num_heads = 8, dropout_rate = 0.1):\n","\n","    self.num_layers = num_layers\n","    self.d_model = num_layers\n","    self.dff = num_layers\n","    self.num_heads = num_layers\n","    self.dropout_rate = dropout_rate\n","\n","    # Optimizer\n","    self.learning_rate = CustomSchedule(d_model)\n","    self.optimizer = tf.keras.optimizers.Adam(self.learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)\n","\n","    # Loss and metrics\n","    self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n","    self.train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n","    self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n","\n","    # Create the model\n","    self.transformer = Transformer(num_layers = self.num_layers, d_model = self.d_model, num_heads = self.num_heads, dff = self.dff,\n","                                  input_vocab_size = self.input_vocab_size, target_vocab_size = self.target_vocab_size, \n","                                  pe_input = self.input_vocab_size, pe_target = self.target_vocab_size, rate = self.dropout_rate)\n","\n","\n","    # Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every n epochs.\n","\n","    checkpoint_path = self.checkpoint_dir + \"/\" + self.input_language + \"_\" + self.target_language \n","    ckpt = tf.train.Checkpoint(transformer = self.transformer, optimizer = self.optimizer)\n","    self.ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","    # if a checkpoint exists, restore the latest checkpoint.\n","    if self.ckpt_manager.latest_checkpoint:\n","      ckpt.restore(ckpt_manager.latest_checkpoint)\n","      print ('Latest checkpoint restored!!')\n","\n","  def restore(self):\n","    # todo : recuperer tout les attributs utiliser par evaluate, translate, plot attention\n","    \"\"\"\n","    self.tokenizer_input_lang\n","    self.tokenizer_target_lang\n","    self.transformer\n","    self.MAX_LENGTH\n","    \"\"\"\n","    #self.transformer = tf.keras.Model.load(self.dir_model+ \"/\" + self.input_language + \"_\" + self.target_language + \".h5\")\n","    pass\n","\n","  \n","\n","  # loss_function\n","  def loss_function(self, real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = self.loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_mean(loss_)\n","\n","  \"\"\"\n","  The @tf.function trace-compiles train_step into a TF graph for faster execution. \n","  The function specializes to the precise shape of the argument tensors. \n","  To avoid re-tracing due to the variable sequence lengths or variable batch sizes (the last batch is smaller), \n","  use input_signature to specify more generic shapes.\n","  \"\"\"\n","  \"\"\"\n","  @tf.function(input_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","  ])\n","  \"\"\"\n","  def train_step(self, inp, tar):\n","    \"\"\"\n","    The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. \n","    tar_real is that same input shifted by 1: At each location in tar_input, tar_real contains the next token that should be predicted.\n","\n","    For example, sentence = \"SOS A lion in the jungle is sleeping EOS\"\n","    tar_inp = \"SOS A lion in the jungle is sleeping\"\n","    tar_real = \"A lion in the jungle is sleeping EOS\"\n","\n","    The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next.\n","    During training this example uses teacher-forcing. Teacher forcing is passing the true output to the next \n","    time step regardless of what the model predicts at the current time step.\n","\n","    As the transformer predicts each word, self-attention allows it to look at the previous words in the input sequence to better predict the next word.\n","    To prevent the model from peaking at the expected output the model uses a look-ahead mask.\n","    \"\"\"\n","    print(inp)\n","    print(\"inp - tar\")\n","    print(tar)\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","    \n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","    print(\"mask ok\")\n","\n","    with tf.GradientTape() as tape:\n","      predictions, _ = self.transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n","      loss = self.loss_function(tar_real, predictions)\n","    print(\"tf.GradientTape() ok\")\n","    gradients = tape.gradient(loss, self.transformer.trainable_variables)    \n","    self.optimizer.apply_gradients(zip(gradients, self.transformer.trainable_variables))\n","    print(\"optimizer ok\")\n","    self.train_loss(loss)\n","    self.train_accuracy(tar_real, predictions)\n","    print(\"train step ok\")\n","\n","  def train(self, EPOCHS=10):\n","    for epoch in range(EPOCHS):\n","      start = time.time()\n","      \n","      self.train_loss.reset_states()\n","      self.train_accuracy.reset_states()\n","      \n","      # inp -> input_language, tar -> target_language\n","      for (batch, (inp, tar)) in enumerate(self.train_dataset):\n","        self.train_step(inp, tar)\n","        \n","        if batch % 100 == 0:\n","          print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","              epoch + 1, batch, self.train_loss.result(), self.train_accuracy.result()))\n","          \n","      if (epoch + 1) % 5 == 0:\n","        ckpt_save_path = self.ckpt_manager.save()\n","        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                             ckpt_save_path))\n","      print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                    self.train_loss.result(), \n","                                                    self.train_accuracy.result()))\n","\n","      print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n","\n","    # todo \n","    # self.transformer.save(self.dir_model+ \"/\" + self.input_language + \"_\" + self.target_language + \".h5\")\n","\n","\n","  def evaluate(self, inp_sentence):\n","    \"\"\"\n","    The following steps are used for evaluation:\n","    - Encode the input sentence using the Portuguese tokenizer (tokenizer_pt). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n","    - The decoder input is the start token == tokenizer_en.vocab_size.\n","    - Calculate the padding masks and the look ahead masks.\n","    - The decoder then outputs the predictions by looking at the encoder output and its own output (self-attention).\n","    - Select the last word and calculate the argmax of that.\n","    - Concatentate the predicted word to the decoder input as pass it to the decoder.\n","    - In this approach, the decoder predicts the next word based on the previous words it predicted.\n","    \"\"\"\n","    start_token = [self.tokenizer_input_lang.vocab_size]\n","    end_token = [self.tokenizer_input_lang.vocab_size + 1]\n","  \n","    # inp sentence is input_language, hence adding the start and end token\n","    inp_sentence = start_token + self.tokenizer_input_lang.encode(inp_sentence) + end_token\n","    encoder_input = tf.expand_dims(inp_sentence, 0)\n","    \n","    # as the target is target_language, the first word to the transformer should be the target_language start token.\n","    decoder_input = [self.tokenizer_target_lang.vocab_size]\n","    output = tf.expand_dims(decoder_input, 0)\n","      \n","    for i in range(self.MAX_LENGTH):\n","      enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","    \n","      # predictions.shape == (batch_size, seq_len, vocab_size)\n","      predictions, attention_weights = self.transformer(inp = encoder_input, \n","                                                   tar = output,\n","                                                   training = False,\n","                                                   enc_padding_mask = enc_padding_mask,\n","                                                   look_ahead_mask = combined_mask,\n","                                                   dec_padding_mask = dec_padding_mask)\n","      \n","      # select the last word from the seq_len dimension\n","      predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","      \n","      # return the result if the predicted_id is equal to the end token\n","      if predicted_id == self.tokenizer_target_lang.vocab_size+1:\n","        return tf.squeeze(output, axis=0), attention_weights\n","      \n","      # concatentate the predicted_id to the output which is given to the decoder as its input.\n","      output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","  def plot_attention_weights(self, attention, sentence, result, layer):\n","    fig = plt.figure(figsize=(16, 8))\n","    \n","    sentence = self.tokenizer_input_lang.encode(sentence)\n","    \n","    attention = tf.squeeze(attention[layer], axis=0)\n","    \n","    for head in range(attention.shape[0]):\n","      ax = fig.add_subplot(2, 4, head+1)\n","      \n","      # plot the attention weights\n","      ax.matshow(attention[head][:-1, :], cmap='viridis')\n","\n","      fontdict = {'fontsize': 10}\n","      \n","      ax.set_xticks(range(len(sentence)+2))\n","      ax.set_yticks(range(len(result)))\n","      \n","      ax.set_ylim(len(result)-1.5, -0.5)\n","          \n","      ax.set_xticklabels(\n","          [SOS]+[self.tokenizer_input_lang.decode([i]) for i in sentence]+[EOS], \n","          fontdict=fontdict, rotation=90)\n","      \n","      ax.set_yticklabels([self.tokenizer_target_lang.decode([i]) for i in result \n","                          if i < self.tokenizer_target_lang.vocab_size], \n","                         fontdict=fontdict)\n","      \n","      ax.set_xlabel('Head {}'.format(head+1))\n","    \n","    plt.tight_layout()\n","    plt.show()\n","\n","  def translate(self, sentence, plot = ''):\n","    result, attention_weights = self.evaluate(sentence)\n","    predicted_sentence = self.tokenizer_target_lang.decode([i for i in result if i < self.tokenizer_target_lang.vocab_size])  \n","\n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(predicted_sentence))\n","    \n","    if plot:\n","      plot_attention_weights(attention_weights, sentence, result, plot)\n","\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jm1mvE9_77FQ","colab_type":"code","colab":{}},"source":["input_language = \"Francais\"\n","target_language = \"Anglais\"\n","path_to_dataset = \"/content/drive/My Drive/datasets/YourVersion/txts/\"+input_language+\"_\" +target_language +\"_pnb1.txt\"\n","vocab_dir = \"/content/drive/My Drive/datasets/YourVersion/vocab_dir\"\n","dir_model = \"/content/drive/My Drive/datasets/YourVersion/dir_model\"\n","checkpoint_dir =  \"/content/drive/My Drive/datasets/YourVersion/checkpoint_dir\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCU3diYk7_06","colab_type":"code","colab":{}},"source":["t = Trainer(\n","    input_language = input_language, \n","    target_language = target_language, \n","    path_to_dataset = path_to_dataset, \n","    vocab_dir = vocab_dir,\n","    dir_model = dir_model,\n","    checkpoint_dir = checkpoint_dir,\n","    num_examples = None\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5wfWCp59fY3","colab_type":"code","outputId":"b5eee9e5-0ad8-4911-8e02-21639edf6d90","executionInfo":{"status":"ok","timestamp":1586250654152,"user_tz":-120,"elapsed":1416,"user":{"displayName":"pascal notsawo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB3JNptAHBvU_EJOwzOH0F7I9P3NvG7oF-0Ae9=s64","userId":"02674667745815558765"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["input_batch, target_batch = next(iter(t.val_dataset))\n","input_batch, target_batch"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(64, 103), dtype=int32, numpy=\n"," array([[ 558,  168,    1, ...,    0,    0,    0],\n","        [  37,   75,  306, ...,    0,    0,    0],\n","        [  19,    9,    8, ...,    0,    0,    0],\n","        ...,\n","        [  19,    9,   67, ...,    0,    0,    0],\n","        [ 575,    1, 2971, ...,    0,    0,    0],\n","        [  19,  706,    1, ...,    0,    0,    0]], dtype=int32)>,\n"," <tf.Tensor: shape=(64, 103), dtype=int32, numpy=\n"," array([[1139,  173,    1, ...,    0,    0,    0],\n","        [  60,  103, 1093, ...,    0,    0,    0],\n","        [  59,   34,  280, ...,    0,    0,    0],\n","        ...,\n","        [  35,   66,  207, ...,    0,    0,    0],\n","        [   3,   43,   18, ...,    0,    0,    0],\n","        [ 153,  123,    1, ...,    0,    0,    0]], dtype=int32)>)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"Ne0j6Ko39Pvc","colab_type":"code","colab":{}},"source":["t.build()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjzSpXuh9XNK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"outputId":"b6dacb60-75a5-40cd-8fef-519dd494e0f0","executionInfo":{"status":"error","timestamp":1586250366075,"user_tz":-120,"elapsed":2066,"user":{"displayName":"pascal notsawo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB3JNptAHBvU_EJOwzOH0F7I9P3NvG7oF-0Ae9=s64","userId":"02674667745815558765"}}},"source":["t.train(EPOCHS=20)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[ 100 5588    4 ...    0    0    0]\n"," [  19   43 1527 ...    0    0    0]\n"," [  19 2967   12 ...    0    0    0]\n"," ...\n"," [ 441 3572   20 ...    0    0    0]\n"," [   9 4028    1 ...    0    0    0]\n"," [  43 2054    7 ...    0    0    0]], shape=(64, 103), dtype=int32)\n","inp - tar\n","tf.Tensor(\n","[[ 337 7963 7894 ...    0    0    0]\n"," [2570 4499 7900 ...    0    0    0]\n"," [ 136 7894    6 ...    0    0    0]\n"," ...\n"," [ 169    9 4653 ...    0    0    0]\n"," [3915  268    1 ...    0    0    0]\n"," [  69 4931  203 ...    0    0    0]], shape=(64, 103), dtype=int32)\n","mask ok\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-ba30104b72b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-36-ed47d5cd3907>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, EPOCHS)\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0;31m# inp -> input_language, tar -> target_language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-ed47d5cd3907>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, inp, tar)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.GradientTape() ok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-e9dc52deae13>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-e9dc52deae13>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;31m# adding embedding and position encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int32'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    135\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         result = _clip(\n\u001b[0;32m--> 137\u001b[0;31m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4520\u001b[0m     \u001b[0;31m# TODO(apassos) find a less bad way of detecting resource variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[0;31m# without introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4523\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4524\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[0;34m(self, indices, name)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[0;32m--> 676\u001b[0;31m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[0;34m(resource, indices, dtype, batch_dims, validate_indices, name)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,15] = 8423 is not in [0, 8113) [Op:ResourceGather]"]}]},{"cell_type":"code","metadata":{"id":"Tm-a45CqAp_l","colab_type":"code","colab":{}},"source":["t.restore()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptxg4h5U8CMk","colab_type":"code","colab":{}},"source":["t.translate(\"je suis l'alpha et l'omega.\")\n","t.translate('le premier et le dernier.')\n","t.translate('le commencement et la fin')\n","t.translate('il a venge le sang de ses serviteurs repandu par ses mains.')\n","t.translate(\"et ils dirent une seconde fois alleluia.\")"],"execution_count":0,"outputs":[]}]}